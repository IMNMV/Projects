#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Jun 24 00:07:41 2023

@author: nyk, GPT-4
"""
from nltk.tokenize import sent_tokenize
import shutil
from datetime import datetime
from pymongo import MongoClient
from sentence_transformers import SentenceTransformer
import numpy as np
import os

# The file (learnings.txt in this case) you want to use for this code to talk to should be in bullet point form and as concise as possible. 

# This code uses the all miniLM transformer model to create sentence embeddings we can computer cosine similarity on.
# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2

# V7 makes a folder for backups to go into and do not create backups if the learning is empty.
# We want it empty because if its full it will duplicate entries into the mongo db and return duplicate answers.
# Adds utilities for managing the database 
# Explains the code more thoroughly

# Connect to MongoDB
client = MongoClient('mongodb://localhost:27018/')
db = client['knowledge_base3']
collection = db['proto1']


'''
General Utilities

# Review documents
for doc in collection.find({'is_question': False}):
    print(doc)

# Delete a specific entry
collection.delete_one({'content': 'content to delete'})

'''


# Initialize the model to create sentence embeddings
model = SentenceTransformer('/Users/nyk/sentence-sim/all-MiniLM-L6-v2')

# Takes a text string as an input and replaces all newline characters (\n) with a space, effectively making the text a single line.
def preprocess_text(text):
    return text.replace('\n', ' ')

# This function calculates the cosine similarity between two vectors a and b by taking their dot product and dividing it by the product of their norms (lengths). 
# This is a measure of how similar the two vectors are.
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


    '''
    The following functions takes a user's prompt and returns the most relevant learnings from the MongoDB collection.
    
    Parameters:
    - user_prompt: The input question from the user.
    - top_n: The maximum number of results to return. Default is 5. 
    - threshold: The minimum cosine similarity to consider a learning as relevant. Default is 0.4. Change as needed. The closer to 1 you go the more strict it becomes.
    
    How it works:
    1. The function encodes the user's prompt into an embedding using the model.
    2. It initializes an empty list to store the similarities of each learning.
    3. The function then iterates through each entry in the MongoDB collection:
        - It skips entries marked as a question.
        - It computes the cosine similarity between the user's prompt and the content embedding of the current entry.
        - If the cosine similarity exceeds the threshold, it appends a tuple of the content and its similarity to the list.
    4. The list of similarities is sorted in descending order.
    5. The function then returns the content of the top_n most similar entries as the most relevant learnings.
    
    Returns:
    - A list of the content of the top_n most relevant learnings.
    '''
    
def get_most_relevant_learnings(user_prompt, top_n=5, threshold=0.4):
    user_prompt_embedding = model.encode([user_prompt])[0]
    learning_similarity = []
    for entry in collection.find():
        if entry.get('is_question', False):
            continue
        content_embedding = np.array(entry['content_embedding'])
        similarity = cosine_similarity(user_prompt_embedding, content_embedding)
        if similarity > threshold:
            learning_similarity.append((entry['content'], similarity))
    
    # Sort by similarity and get the top_n
    learning_similarity.sort(key=lambda x: x[1], reverse=True)
    most_relevant_learnings = [learning for learning, similarity in learning_similarity[:top_n]]
    
    return most_relevant_learnings


# This function computes a numerical representation (embedding) of a given text using the model, checks if the text is already in the database.
# If it's not, stores the text and its corresponding embedding in the MongoDB collection.
def compute_and_store_embedding(text):
    # Compute the embedding for the text
    embedding = model.encode([text])[0].tolist()

    # Check if this text is already in the database
    if collection.count_documents({'content': text}, limit = 1) == 0:
        # If it's not, add it to the database
        collection.insert_one({'content': text, 'content_embedding': embedding, 'is_question': False})
    else:
        print(f"Entry '{text}' already exists in the database.")



# Update the database with new learnings
with open('/Users/nyk/Desktop/proto/learnings.txt', 'r') as file:
    learnings = file.read()

sentences = sent_tokenize(learnings)

for sentence in sentences:
    sentence = preprocess_text(sentence)
    compute_and_store_embedding(sentence)


# Get the directory of 'learnings.txt'
learnings_dir = os.path.dirname('/Users/nyk/Desktop/proto/learnings.txt')

# Check if learnings.txt is empty - if it is do not create a back up
if os.stat("/Users/nyk/Desktop/proto/learnings.txt").st_size != 0:
    # Create a backup before clearing the file
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_filename = f'learnings_backup_{timestamp}.txt'

    # Form the path of the backup file
    backup_path = os.path.join(learnings_dir, 'backups', backup_filename)

    shutil.copy('/Users/nyk/Desktop/proto/learnings.txt', backup_path)

    # Clear the file after creating a backup
    open('/Users/nyk/Desktop/proto/learnings.txt', 'w').close()
else:
    print("Learnings file is empty. No backup created.")



# Question for which you want to search the database for
question = input("Enter a question: ")  

# Retrieve the most relevant context for a question
most_relevant_learning = get_most_relevant_learnings(question)
print(f"Most relevant learning: {most_relevant_learning}")
