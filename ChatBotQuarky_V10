#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Apr 16 21:42:24 2023
@author: nyk, GPT-4
credit to @yoheinakajima (twitter handle) for this concept of feed forward agents to improve self refinement of responses.
python 3.10
It chose the name Quarky - not me.
"""

# decent follow up context without specific prompting if the conversation is specific enough. 
# may still have issue refining context and needs the out going ount token logic to apply
# need to clean up text eyes more so it fits new structure of speaking to API

# updates here:
    # fixed the recent responses which were getting cut off by outgoing tokens with this line   if entry["response"] != "":
          # last_non_empty_responses.append({"prompt": entry["prompt"], "response": entry["response"]})

# much better context with previous convos without providing a ton of qualifiers in the prompt for it to understand even with 3.5s less powerful reasoning
        # "content": f"Think about the request from the user given the context, think about those thoughts then respond to the following: {user_prompt}\n. Make your responses in the format of Initial Thoughts of Prompt: \n Refined Thoughts: \n Response to the User:"


# honestly the output from process screenshot is pretty good. you might make that the text eyes output instead not to confuse the model also increases speed
# removed it - seems decent

# V11 should utilize the newest update from OpenAI which allows functions to be used inside the API call.


'''
ANSI escape codes for colors reference
Foreground colors:

    Black : 30
    Red : 31
    Green : 32
    Yellow : 33
    Blue : 34
    Magenta : 35
    Cyan : 36
    White : 37
    
    Background colors:

    Black : 40
    Red : 41
    Green : 42
    Yellow : 43
    Blue : 44
    Magenta : 45
    Cyan : 46
    White : 47
    
'''
    

import openai
import json
import numpy as np
from pymongo import MongoClient
import time
import subprocess
import os
import pytesseract
from PIL import Image
import requests
import re
from mss import mss
import tiktoken

def count_outgoing_tokens(text, model="gpt-3.5-turbo"):
    # Get the encoding for a specific model
    enc = tiktoken.encoding_for_model(model)

    # Encode the text
    tokens = enc.encode(text)

    # Return the number of tokens
    return len(tokens)

def count_tokens(text):
    pattern = r'\w+|[^\w\s]'
    tokens = re.findall(pattern, text)
    return len(tokens)


# yeah yeah yeah I know I shouldn't hard code it here but I'm too lazy to fix it right now
OPENAI_API_KEY = ""

with open('/Users/nyk/Desktop/apikeystorage/api_key', 'r') as f:
    api_key = f.read().strip()


openai.api_key_path = "/Users/nyk/Desktop/apikeystorage/api_key"


client = MongoClient('mongodb://localhost:#####/')
db = client["Refine"]
collection = db["Production3"]
selected_model = "gpt-3.5-turbo"

DANGEROUS_COMMANDS = [
    r"rm -rf",
    r"rm",
    r"rm -r",
    r"dd if=/dev/zero of=",
    r"mkfs\..*",
    r"wget .* \| sh",
    r"chmod -R 777 /",
    r"forkbomb\(\)",
    r"iptables -F",
]

def execute_command(command):
    script_filename = "gpt3_commands.sh"

    if any(re.match(pattern, command) for pattern in DANGEROUS_COMMANDS):
        user_input = input("\033[31mARE YOU SURE YOU WANT TO EXECUTE THIS POTENTIALLY DANGEROUS COMMAND??? IF NOT THIS MODEL IS FUCKING WITH YOU. YOU BETTER UNPLUG THAT SHIT. HOWEVER, IF YOU ARE SURE WRITE:\033[32m Yes, it is perfectly reasonable to execute this command.\033[0m Write your confirmation now:")

        if user_input != "Yes, it is perfectly reasonable to execute this command.":
            print("Command not executed. User did not confirm execution.")
            return

    with open(script_filename, "w") as script_file:
        script_file.write("#!/bin/bash\n")
        script_file.write(command)

    with open(script_filename, "r") as script_file:
        print(script_file.read())

    os.chmod(script_filename, 0o755)
    os.system(f"open -a Terminal {script_filename}")



def color_text(text, color_code):
    return f'\033[{color_code}m{text}\033[0m'


def get_embeddings_and_store(prompt, response):
    prompt_embedding = openai.Embedding.create(
        input=prompt,
        model="text-embedding-ada-002"
    )['data'][0]['embedding']

    response_embedding = openai.Embedding.create(
        input=response,
        model="text-embedding-ada-002"
    )['data'][0]['embedding']

    conversation_entry = {
        'prompt': prompt,
        'response': response,
        'prompt_embedding': prompt_embedding,
        'response_embedding': response_embedding
    }

    collection.insert_one(conversation_entry)

    return prompt_embedding, response_embedding


def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


def get_most_relevant_context(user_prompt_embedding, recently_used_responses, threshold=0.8, max_tokens=1800):
    relevant_context = []
    last_non_empty_responses = []
    accumulated_tokens = 0

    recently_used_tokens = sum(count_outgoing_tokens(f"{entry['prompt']} {entry['response']}") for entry in recently_used_responses)
    max_tokens -= recently_used_tokens  # Reserve tokens for the recent responses

    # Add recent responses to the relevant_context first
    for entry in recently_used_responses:
        relevant_context.append(entry)
        accumulated_tokens += count_outgoing_tokens(f"{entry['prompt']} {entry['response']}")

    for entry in collection.find():
        prompt_embedding = np.array(entry['prompt_embedding'])
        response_embedding = np.array(entry['response_embedding'])

        prompt_similarity = cosine_similarity(user_prompt_embedding, prompt_embedding)
        response_similarity = cosine_similarity(user_prompt_embedding, response_embedding)

        current_entry_tokens = count_outgoing_tokens(f"{entry['prompt']} {entry['response']}")
        if (prompt_similarity > threshold or response_similarity > threshold) and entry not in recently_used_responses:
            # Ensure max_tokens limit is applied only to the entries fetched from the database
            if accumulated_tokens + current_entry_tokens <= max_tokens:
                relevant_context.insert(0, entry)  # Add associative context to the start of the list
                accumulated_tokens += current_entry_tokens
            elif accumulated_tokens >= max_tokens:  # Stop adding to context when max_tokens is hit
                break

        if entry["response"] != "":
            last_non_empty_responses.append({"prompt": entry["prompt"], "response": entry["response"]})

    return relevant_context, last_non_empty_responses


def refine_context(api_key, context):
    url = 'https://api.openai.com/v1/chat/completions'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    payload = {
        "model": "gpt-3.5-turbo",
        "messages": [
            {"role": "system", "content": "You are a refinement AI. Refine the following context to be more specific without losing important information. Such as names given, pathways to documents, games played, or general tone of conversation."},
            {"role": "user", "content": f"Please refine {context}"}
        ]
    }
    response = requests.post(url, headers=headers, data=json.dumps(payload))
    response_data = response.json()

    # Print the response data for debugging purposes
    print("Response data:", response_data)

    try:
        refined_context = response_data["choices"][0]["message"]["content"].strip()
    except KeyError:
        print("Error: 'choices' key not found in the response data. Using the original context.")
        refined_context = context

    return refined_context




def chat_gpt_response(api_key, selected_model, user_prompt, recently_used_responses=[], relevant_context=None, last_n_responses=3):   
    user_prompt_embedding, _ = get_embeddings_and_store(user_prompt, "")
    relevant_context, recent_responses = get_most_relevant_context(user_prompt_embedding, recently_used_responses)
    recent_responses = recent_responses[-last_n_responses:]  # Keep only the last n responses
    prompt = f"Based on the context that was just sent to you, respond to the following prompt: {user_prompt}\n. Make your responses in the format of Initial Thoughts of Prompt: \n Refined Thoughts: \n Response to the User:"



    #relevant_context += recent_responses
    # Refine and summarize context before using it
    context_string = ' '.join([f"{entry['prompt']} {entry['response']}" for entry in relevant_context])
    print("BEGINNING OF CONTEXT STRING")
    print(context_string)
    print("END OF CONTEXT STRING")
    print("BEGINNING OF RECENT RESPONSES")
    print(recent_responses)
    print("END OF RECENT RESPONSES")
    # Check if the context string has more than 3000 chars 
   # if len(context_string.split()) > 3400:
       # context_string = refine_context(api_key, context_string)

    url = 'https://api.openai.com/v1/chat/completions'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }

    payload = {
        "model": "gpt-3.5-turbo",
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful, witty, and extremely extremely sarcastic AI. If you don't know the answer to a question, make an educated guess and advise the user to take it with a grain of salt."
            },
            {
                "role": "system",
                "content": "You have the special ability to see anything that is on the current screen of Nykko's computer. To see it say any of the following keyword: 'eyes'."
            },
            {
                "role": "system",
                "content": "The following is a collection of previous conversations pulled from a database based on relevance to the users prompt listed from oldest to newest. Please consider these when formulating your responses."
            },
            {
                "role": "user",
                "content": context_string
            },
            {
                "role": "system",
                "content": f"The following are the last 3 recent interactions we have had. Use this for context to all questions asked. Most Recent Responses: {recent_responses}"
            },
            
            {
                "role": "user",
                #"content": "Based on the previous conversations and responses provided in the context, think about the user's request. Respond in the following format: Initial Thoughts | Refined Thoughts: | Responses to the User: "
                "content": prompt

                
                }
        ],
        "temperature": .03
    }


    print(payload)
    response = requests.post(url, headers=headers, data=json.dumps(payload))
    response.raise_for_status()
    tokens_used = response.json()['usage']['total_tokens']
    
    print(color_text(f"Tokens used for this API call: {tokens_used}", 35))
    output_text = response.json()["choices"][0]["message"]["content"]

    if len(recently_used_responses) >= last_n_responses:
        recently_used_responses.pop(0)
    recently_used_responses.append({"prompt": user_prompt, "response": output_text})

    
    # Store the API response alongside the user prompt
    get_embeddings_and_store(user_prompt, output_text)

    #print(color_text("*****Initial GPT Response*****", 36))
    #print(output_text)
    #print(color_text("*****END OF GPT Response*****", 36))

    return output_text, relevant_context



def coding_agent(api_key, user_prompt, output_from_chatgpt_response, recently_used_responses, relevant_context) -> str:
    system_prompt = (f"You are an AI that takes the output from another AI, removes any symbols that would not work with the execute() function and return that code that is now cleaned and able to be executed using Unix by default unless another language is mentioned. That is, write nothing but the code so it can be executed in my console and carried out automatically without user intervention. The following output you are to perform this task is as follows: {output_from_chatgpt_response}.\n\n")              
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=system_prompt,
        max_tokens=100,
        n=1,
        stop=None,
        temperature=.0001,
    )

    output_text = response.choices[0].text.strip()

    return output_text

recently_used_responses = []

rerun_script = False
KEYWORDS = ["cd", "/Users/", "#bin", "/Applications/", "import", "Firefox", "Safari"]
#KEYWORDS = ["rw4serthdxyt"]

def process_screenshot(api_key):


    # Capture screenshot
    sct = mss()
    screenshot_path = '/Users/nyk/Desktop/screenshot.png'
    sct.shot(output=screenshot_path)

    # Use OCR to convert image to text
    text = pytesseract.image_to_string(Image.open(screenshot_path))

    # Clean up text
    cleaned_text = re.sub(r'\W+', ' ', text)
    print(cleaned_text)

    # Send data to GPT API
    url = 'https://api.openai.com/v1/chat/completions'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }

    payload = {
        "model": "gpt-3.5-turbo",
        #"model": "gpt-4",
        "messages": [
            {"role": "system", "content": "You are an AI whose job is it to summarize and explain a screenshot taken from a computer screen. The image has been processed with Optical Character Recognition (OCR) to convert the visual data into textual data. This text might include various types of information and could potentially contain errors due to the OCR process - try your best nonetheless. Your task is to interpret this text and provide the best response you can."},
            {"role": "user", "content": f"Here is the text from the screen shot to explain from OCR: {cleaned_text}"}
        ],
        "temperature": .01
    }

    #response = requests.post(url, headers=headers, json=payload)
    response = requests.post(url, headers=headers, data=json.dumps(payload))
    response.raise_for_status()
    gpt_response = response.json()['choices'][0]['message']['content']
    print(gpt_response)
    
    
    
    response = requests.post(url, headers=headers, data=json.dumps(payload))
    response.raise_for_status()
    tokens_used = response.json()['usage']['total_tokens']
    
    print(color_text(f"Tokens used for this API call: {tokens_used}", 35))
    output_text = response.json()["choices"][0]["message"]["content"]
    return gpt_response

EYE_KEYWORDS = ['eyes', 'screenshot']




while True:
    if not rerun_script:
        user_prompt = input("Type your input here: ")
    rerun_script = False
    chat_gpt_response_text, relevant_context = chat_gpt_response(api_key, selected_model, user_prompt, recently_used_responses, last_n_responses=3)

    executed = False  # To track if any action was executed

    # Check if any keyword is present in the chat_gpt_response_text
    if any(keyword in chat_gpt_response_text for keyword in KEYWORDS):
        # Call the coding_agent function with the output of chat_gpt_response
        coding_agent_response_text = coding_agent(api_key, user_prompt, chat_gpt_response_text, recently_used_responses, relevant_context)
        print(f"*******Da Vinci's Code********")
        print(f"******{coding_agent_response_text}********")

        # Update recently_used_responses after getting the output_text
        if len(recently_used_responses) >= 3:
            recently_used_responses.pop(0)
        recently_used_responses.append({"prompt": user_prompt, "response": coding_agent_response_text})

        # Execute the code generated by the free_will function using the execute_command function
        execute_command(coding_agent_response_text)
        executed = True

    if any(keyword in chat_gpt_response_text for keyword in EYE_KEYWORDS):
        screenshot_text = process_screenshot(api_key)

        print(f"*******Text Eyes GPT Response ********")
        print(f"*******{screenshot_text}********")

        # Update recently_used_responses after getting the output_text
        if len(recently_used_responses) >= 3:
            recently_used_responses.pop(0)
        recently_used_responses.append({"prompt": user_prompt, "response": screenshot_text})
        executed = True

    if not executed:
        # If neither keyword was found, just print the GPT response
        print(f"*******Chat GPT Response ********")
        print(f"*******{chat_gpt_response_text}********")

        # Update recently_used_responses after getting the output_text
        if len(recently_used_responses) >= 3:
            recently_used_responses.pop(0)
        recently_used_responses.append({"prompt": user_prompt, "response": chat_gpt_response_text})
