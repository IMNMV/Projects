#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Mar 23 21:16:45 2023

@author: nyk, GPT-4
"""
#unified location to store all my most useful stuff. Started with a unified spot to pull open AI models but started adding others.
#stable diffusion, and my custom code executer version of text davinci 003 which can execute unix code ... if prompted correctly. Has some functionality to control your system.
#eventually will become the cognitive hub jarvis uses to interact with all my systems

import json
import openai
import requests
import tkinter as tk
import threading
import time
import cv2
from diffusers import StableDiffusionPipeline
from PIL import Image, ImageTk
import tkinter.ttk as ttk
import tkinter as tk
from tkinter import ttk
import tkinter.font as tkFont
import openai
import os
import subprocess
from tkinter import messagebox



preprompt = "Any code written will always begin with Unix syntax. Never use interactive text editors. My pathway starts with /Users/nyk/"

# Function to send the prompt to GPT-3
def send_prompt_to_gpt3(prompt):
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=prompt,
        max_tokens=100,
        n=1,
        stop=None,
        temperature=0.5,
    )
    return response.choices[0].text.strip()

# Function to execute the command
def execute_command(command):
    script_filename = "gpt3_commands.sh"
    
    with open(script_filename, "w") as script_file:
        script_file.write("#!/bin/bash\n")
        script_file.write(command)
        #script_file.write("\nexit\n")
    
    with open(script_filename, "r") as script_file:
        print(script_file.read())
    
    os.chmod(script_filename, 0o755)
    os.system(f"open -a Terminal {script_filename}")


with open("/Users/nyk/Desktop/apikeystorage/api_key", "r") as file:
    api_key = file.read().strip()

with open("/Users/nyk/Desktop/apikeystorage/api_key", "r") as file:
    api_key = file.read().strip()

# Define a list of available models
models = [
    "gpt-4",
    "gpt-4-0314",
    "gpt-3.5-turbo",
    "gpt-3.5-turbo-0301",
    "Stable Diffusion",
    "text-davinci-003",
    "CodeController",
    "babbage",
    "davinci",
    "babbage-code-search-code",
    "text-similarity-babbage-001",
    "text-davinci-001",
    "ada",
    "curie-instruct-beta",
    "babbage-code-search-text",
    "babbage-similarity",
    "whisper-1",
    "code-search-babbage-text-001",
    "text-curie-001",
    "code-search-babbage-code-001",
    "text-ada-001",
    "text-embedding-ada-002",
    "text-similarity-ada-001",
    "ada-code-search-code",
    "ada-similarity",
    "code-search-ada-text-001",
    "text-search-ada-query-001",
    "davinci-search-document",
    "ada-code-search-text",
    "text-search-ada-doc-001",
    "davinci-instruct-beta",
    "text-similarity-curie-001",
    "code-search-ada-code-001",
    "ada-search-query",
    "text-search-davinci-query-001",
    "curie-search-query",
    "davinci-search-query",
    "babbage-search-document",
    "ada-search-document",
    "text-search-curie-query-001",
    "text-search-babbage-doc-001"
    "text-search-davinci-doc-001",
    "text-search-babbage-query-001",
    "curie-similarity",
    "curie",
    "text-similarity-davinci-001",
    "text-davinci-002",
    "davinci-similarity",
    "cushman:2020-05-03",
    "ada:2020-05-03",
    "babbage:2020-05-03",
    "curie:2020-05-03",
    "davinci:2020-05-03",
    "if-davinci-v2",
    "if-curie-v2",
    "if-davinci:3.0.0",
    "davinci-if:3.0.0",
    "davinci-instruct-beta:2.0.0",
    "text-ada:001",
    "text-davinci:001",
    "text-curie:001",
    "text-babbage:001",
    "ada:ft-personal-2023-03-04-06-20-07",
    "ada:ft-personal-2023-03-03-05-07-30",
]

                
class App:
    def __init__(self, root):
        self.root = root
        self.root.title("OpenAI API Demo")
        self.root.geometry("800x600")
        self.root.configure(bg='#2E3440')  # Change the background color

        # Create a frame for the model selection and generate button
        self.top_frame = tk.Frame(self.root, bg='#2E3440')
        self.top_frame.pack(pady=(10, 10))

        # Create a dropdown menu for model selection
        self.model_selection = tk.StringVar()
        self.model_selection.set(models[0])  # Set the default model
        self.model_label = tk.Label(self.top_frame, text="Select a model:", bg='#2E3440', fg='green')  # Change the text color
        self.model_label.pack(side='left')

        style = ttk.Style()
        style.configure("TMenubutton", background="#4C566A", foreground="red", highlightthickness=0, bd=0)  # Set the style for the dropdown menu
        self.dropdown_menu = ttk.OptionMenu(self.top_frame, self.model_selection, *models, style="TMenubutton")  # Update the OptionMenu widget
        self.dropdown_menu.pack(side='left')

        # Add a button to generate the text
        self.generate_button = tk.Button(self.top_frame, text="Generate", command=self.generate_text, height=2, width=10)  # Increase the size of the button
        self.generate_button.pack(side='left', padx=(10, 0))

        # Create a textbox for prompt input with a scrollbar
        self.prompt_label = tk.Label(root, text="Enter prompt:", bg='#2E3440', fg='green')
        self.prompt_label.pack()
        default_font = tkFont.nametofont("TkDefaultFont")
        default_font.config(size=14)
        self.prompt_textbox_frame = tk.Frame(root)
        self.prompt_textbox_frame.pack()
        self.scrollbar = tk.Scrollbar(self.prompt_textbox_frame)
        self.scrollbar.pack(side='right', fill='y')
        self.prompt_textbox = tk.Text(self.prompt_textbox_frame, height=40, width=130, font=default_font, yscrollcommand=self.scrollbar.set)
        self.prompt_textbox.pack(side='left')
        self.scrollbar.config(command=self.prompt_textbox.yview)

        # Add a label to display the status
        self.status_label = tk.Label(self.root, text="", bg='#2E3440', fg='red')  # Change the text color
        self.status_label.pack()

        # Add a label to display the output
        self.output_label = tk.Label(self.root, text="Output:", bg='#2E3440', fg='white')  # Change the text color
        self.output_label.pack()

        # Add an image container to display the generated image
        self.image_container = tk.Label(self.root)
        self.image_container.pack()

    def insert_text_in_prompt(self, text, tag):
        """
        Insert text into the prompt textbox with the specified tag.
        """
        self.prompt_textbox.insert(tk.END, text + "\n")
        start = f"{float(self.prompt_textbox.index(tk.END)) - 1.0} linestart"
        end = f"{float(self.prompt_textbox.index(tk.END)) - 1.0} lineend"
        self.prompt_textbox.tag_add(tag, start, end)
        self.prompt_textbox.tag_configure(tag, foreground="white")

        
        if tag == "gpt":
            # Set the color for the current GPT response to green
            self.prompt_textbox.tag_configure(tag, foreground="green")
            
            # Set the color for the previous GPT response to yellow
            previous_gpt_start = f"{float(self.prompt_textbox.index(start)) - 2.0} linestart"
            previous_gpt_end = f"{float(self.prompt_textbox.index(start)) - 2.0} lineend"
            self.prompt_textbox.tag_add("previous_gpt", previous_gpt_start, previous_gpt_end)
            self.prompt_textbox.tag_configure("previous_gpt", foreground="yellow")
            
            # Set the color for the previous user response to red
            previous_user_start = f"{float(self.prompt_textbox.index(start)) - 4.0} linestart"
            previous_user_end = f"{float(self.prompt_textbox.index(start)) - 4.0} lineend"
            self.prompt_textbox.tag_add("previous_user", previous_user_start, previous_user_end)
            self.prompt_textbox.tag_configure("previous_user", foreground="red")
        elif tag == "user":
            self.prompt_textbox.tag_configure(tag, foreground="white")
 

    def generate_text(self):
        prompt = self.prompt_textbox.get("1.0", "end-1c")
        selected_model = self.model_selection.get()
        self.prompt_textbox.delete("1.0", "end")
    
        if selected_model == "CodeController":
            openai.api_key = api_key
            output_text = send_prompt_to_gpt3(preprompt + prompt)
            self.insert_text_in_prompt(f"User: {prompt}", "user")
            self.insert_text_in_prompt(f"GPT: {output_text}", "gpt")
            self.prompt_textbox.tag_configure("GPT", foreground="green")
            execute_command(output_text)
        elif "gpt-3.5" in selected_model or "gpt-4" in selected_model:
              url = 'https://api.openai.com/v1/chat/completions'
              headers = {
                  'Content-Type': 'application/json',
                  'Authorization': f'Bearer {api_key}'
              }
              payload = {
                  "model": selected_model,
                  "messages": [{"role": "user", "content": prompt}]
              }
      
              response = requests.post(url, headers=headers, data=json.dumps(payload))
              response.raise_for_status()
              output_text = response.json()["choices"][0]["message"]["content"]
              self.prompt_textbox.insert("end", f"User: {prompt}\n", "user")
              self.prompt_textbox.tag_configure("GPT", foreground="green")
              self.insert_text_in_prompt(f"GPT: {output_text}", "gpt")
        elif selected_model == "Stable Diffusion":
            self.status_label.config(text="Processing... Please wait 50 seconds.")
            generate_image(prompt, self.status_label, self.image_container, self.root, self.prompt_textbox)
        else:
            openai.api_key = api_key
            response = openai.Completion.create(
                engine=selected_model,
                prompt=prompt,
                max_tokens=100,
                n=1,
                stop=None,
                temperature=0.5,
            )
            output_text = response.choices[0].text
            self.insert_text_in_prompt(f"User: {prompt}", "user")
            self.insert_text_in_prompt(f"GPT: {output_text}", "gpt")
            self.prompt_textbox.tag_configure("GPT", foreground="green")


            
# Create the status label widget


def generate_image(prompt, status_label, image_container, root, prompt_textbox):
    model_path = "/Users/nyk/stable-diffusion-v1-5"
    pipe = StableDiffusionPipeline.from_pretrained(model_path)

    pipe = pipe.to("mps")  # Use the 'mps' backend for Apple Silicon devices

    # Recommended if your computer has < 64 GB of RAM
    pipe.enable_attention_slicing()

    # Display a status message while the image is being generated
    status_label.config(text="Processing... Please wait 50 seconds.")

    # Generate the image
    _ = pipe(prompt, num_inference_steps=1)
    image = pipe(prompt, num_inference_steps=100).images[0]

    # Save the image to a file
    image_path = "/Users/nyk/Desktop/stableimages/astronaut_rides_horses.png"
    image.save(image_path)

    # Upscale and denoise the image
    image = Image.open(image_path)
    image_resized = image.resize((1024, 1024), Image.LANCZOS)
    image_resized.save("/Users/nyk/Desktop/stableimages/astronaut_rides_horse_resized.png")

    image = cv2.imread(image_path)
    image_denoised = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)
    cv2.imwrite("/Users/nyk/Desktop/stableimages/astronaut_rides_horse_denoised.png", image_denoised)

    # Display the image in the UI
    img = Image.open(image_path)
    photo = ImageTk.PhotoImage(img)
    image_container.config(image=photo)
    image_container.image = photo

    # Define the function to update the status message
    def update_status_message(i):
        status_label.config(text=f"Processing... Please wait {i} seconds.")
        if i < 50:
            root.after(1000, lambda: update_status_message(i+1))

    # Clear the status message
    status_label.config(text="")
    update_status_message(1)

    # Generate the image
    prompt = prompt_textbox.get("1.0", "end-1c")
    model_path = "/Users/nyk/stable-diffusion-v1-5"
    pipe = StableDiffusionPipeline.from_pretrained(model_path)

    pipe = pipe.to("mps")  # Use the 'mps' backend for Apple Silicon devices

    # Recommended if your computer has < 64 GB of RAM
    pipe.enable_attention_slicing()

    # First-time "warmup" pass (see explanation above)
    _ = pipe(prompt, num_inference_steps=1)

    # Results match those from the CPU device after the warmup pass.
    image = pipe(prompt, num_inference_steps=100).images[0]

    # Save and display the generated image
    image_path = "/Users/nyk/Desktop/stableimages/astronaut_rides_horses.png"
    image.save(image_path)



    # Upscale and denoise the image
    image = Image.open(image_path)
    image_resized = image.resize((1024, 1024), Image.LANCZOS)
    image_resized.save("/Users/nyk/Desktop/stableimages/astronaut_rides_horse_resized.png")

    image = cv2.imread(image_path)
    image_denoised = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)
    cv2.imwrite('/Users/nyk/Desktop/stableimages/astronaut_rides_horse_denoised.png', image_denoised)


if __name__ == "__main__":
    root = tk.Tk()
    app = App(root)
    root.mainloop()
           
