#Face, eye, and movement tracking

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Mar  5 22:11:47 2023

@author: nykv
"""


#issue prints i detected someone a lot then lags

#movement tracking
import os
import cv2
import os

os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/Users/nykv/Desktop/translate/deft-idiom-372802-3812d9fa81a3.json"

from gtts import gTTS


def detect_person(centroids, threshold=10):
    if len(centroids) > threshold and not detect_person.detected:
        print("I detect someone!")
        detect_person.detected = True
    elif len(centroids) <= threshold:
        detect_person.detected = False

detect_person.detected = False

        
# Initialize the video capture object for the built-in camera
cap = cv2.VideoCapture(0)

# Initialize the background subtractor object
backSub = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=50, detectShadows=True)

while True:
    centroids = []
    # Capture a frame from the camera
    ret, frame = cap.read()

    # Apply background subtraction to detect motion
    fgMask = backSub.apply(frame)

    # Apply thresholding to create a binary mask
    thresh = cv2.threshold(fgMask, 127, 255, cv2.THRESH_BINARY)[1]

    # Apply morphological operations to remove noise and fill gaps
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    thresh = cv2.erode(thresh, kernel, iterations=1)
    thresh = cv2.dilate(thresh, kernel, iterations=2)

    # Find contours in the binary mask
    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # For each contour detected, track the centroid and draw a rectangle around it
    for c in contours:
        # Compute the centroid of the contour
        M = cv2.moments(c)
        if M["m00"] == 0:
            continue
        cx = int(M["m10"] / M["m00"])
        cy = int(M["m01"] / M["m00"])
        centroid = (cx, cy)
        centroids.append(centroid)

        # Draw the object ID and centroid on the frame
        cv2.circle(frame, centroid, 4, (0, 255, 0), -1)
        cv2.putText(frame, "centroid", (centroid[0] - 10, centroid[1] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Display the frame in a window
    cv2.imshow('frame', frame)
    
    detect_person(centroids)
    if detect_person.detected:
        response = "Hello! I detected someone!"
        tts = gTTS(text=response, lang='en')
        tts.save("response.mp3")
        os.system("afplay response.mp3")


    # Exit the program if the 'q' key is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the video capture object and destroy the window
cap.release()
cv2.destroyAllWindows()








'''
#eye tracking
import cv2

# Load the pre-trained face and eye detection models
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

# Initialize the video capture object for the built-in camera
cap = cv2.VideoCapture(0)

while True:
    # Capture a frame from the camera
    ret, frame = cap.read()

    # Convert the frame to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Detect faces in the grayscale frame
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    # For each face detected, detect eyes and draw a rectangle around them
    for (x, y, w, h) in faces:
        # Extract the face ROI from the grayscale frame
        roi_gray = gray[y:y + h, x:x + w]

        # Detect eyes in the face ROI
        eyes = eye_cascade.detectMultiScale(roi_gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

        # For each eye detected, draw a rectangle around it
        for (ex, ey, ew, eh) in eyes:
            cv2.rectangle(frame, (x + ex, y + ey), (x + ex + ew, y + ey + eh), (0, 255, 0), 2)

    # Display the frame in a window
    cv2.imshow('frame', frame)

    # Exit the program if the 'q' key is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the video capture object and destroy the window
cap.release()
cv2.destroyAllWindows()

'''









'''
#trying to fix anger - not working - should see emotions
import cv2

# Load the pre-trained face and emotion detection models
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
emotion_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# Initialize the video capture object for the built-in camera
cap = cv2.VideoCapture(0)

# Define a dictionary of emotion labels and corresponding colors
EMOTIONS = {
    "Angry": (0, 0, 255),
    "Disgust": (0, 255, 0),
    "Fear": (255, 255, 0),
    "Happy": (255, 0, 0),
    "Neutral": (255, 255, 255),
    "Sad": (255, 165, 0),
    "Surprise": (0, 255, 255)
}

while True:
    # Capture a frame from the camera
    ret, frame = cap.read()

    # Convert the frame to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Detect faces in the grayscale frame
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    # For each face detected, detect emotions and display the emotion label
    for (x, y, w, h) in faces:
        # Extract the face ROI from the grayscale frame
        roi_gray = gray[y:y + h, x:x + w]

        # Detect emotions in the face ROI
        emotions = emotion_classifier.detectMultiScale(roi_gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

        # For each emotion detected, display the emotion label
        for (ex, ey, ew, eh) in emotions:
            label = "Unknown"
            for emotion in EMOTIONS.keys():
                if emotion in EMOTIONS:
                    label = emotion
                    break
            color = EMOTIONS[label]
            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

        # Draw a rectangle around the face in the original color frame
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

    # Display the frame in a window
    cv2.imshow('frame', frame)

    # Exit the program if the 'q' key is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the video capture object and destroy the window
cap.release()
cv2.destroyAllWindows()
'''



'''
#detects a smile
import cv2

# Load the pre-trained face and emotion detection models
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
emotion_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')

# Initialize the video capture object for the built-in camera
cap = cv2.VideoCapture(0)

while True:
    # Capture a frame from the camera
    ret, frame = cap.read()

    # Convert the frame to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Detect faces in the grayscale frame
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    # For each face detected, detect emotions and display the emotion label
    for (x, y, w, h) in faces:
        # Extract the face ROI from the grayscale frame
        roi_gray = gray[y:y + h, x:x + w]

        # Detect emotions in the face ROI
        emotions = emotion_classifier.detectMultiScale(roi_gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

        # Display the emotion label for the face
        for (ex, ey, ew, eh) in emotions:
            cv2.putText(frame, "Emotion: Smile", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

        # Draw a rectangle around the face in the original color frame
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

    # Display the frame in a window
    cv2.imshow('frame', frame)

    # Exit the program if the 'q' key is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the video capture object and destroy the window
cap.release()
cv2.destroyAllWindows()

'''














'''
#detects a face
import cv2

# Load the pre-trained face detection model
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# Initialize the video capture object for the built-in camera
cap = cv2.VideoCapture(0)

while True:
    # Capture a frame from the camera
    ret, frame = cap.read()

    # Convert the frame to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Detect faces in the grayscale frame
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    # If a face is detected, print "face found" and stop the video capture
    if len(faces) > 0:
        print("face found")
        break

    # Display the frame in a window
    cv2.imshow('frame', frame)

    # Exit the program if the 'q' key is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the video capture object and destroy the window
cap.release()
cv2.destroyAllWindows()
'''
